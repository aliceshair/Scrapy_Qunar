
# ä¸€çº§é¡µé¢, æ¯ä¸ªé¡µé¢æœ‰10æ¡æ™¯ç‚¹, å‡è®¾çˆ¬å–50é¡µ
# åŒ—äº¬æ—…æ¸¸æ™¯ç‚¹åˆ—è¡¨ https://travel.qunar.com/p-cs299914-beijing-jingdian
#------------------------------------------------------------------------------

# æ¯æ¬¡ä¿å­˜
# //åˆ’æ‰QAQ ç»çº¬åº¦(ä¸ä¿å­˜ç™½ä¸ä¿å­˜ ä¸‡ä¸€æœ‰ç”¨å‘¢) : <li class="item" data-lat="39.502139" data-lng="116.340954">
# æ™¯ç‚¹åç§°: å¦‚"æ•…å®«"; <span class="cn_tit">æ•…å®«<span class="en_tit">Forbidden City</span></span>
# æ™¯ç‚¹ç®€ä»‹: ä¸–ç•Œ....; <div class="desbox">ä¸–ç•Œäº”å¤§å®«ä¹‹é¦–ï¼Œæ˜¯ä¸­å›½æ˜æ¸…ä¸¤ä»£çš„çš‡å®¶å®«æ®¿ã€‚</div>
# è¯„è®ºé“¾æ¥ ç‚¹å‡»è¿›å…¥ <a data-beacon="poi" href="https://travel.qunar.com/p-oi710603-gugong"  alt="æ•…å®«"
#------------------------------------------------------------------------------

# äºŒçº§é¡µé¢, æ¯ä¸ªé¡µé¢æœ‰10æ¡è¯„è®º, æ¯æ¬¡è¦ç‚¹å‡»è¿›å…¥ä¸‹ä¸€é¡µ, å¦‚æœæ²¡æœ‰ next, åˆ™è¡¨ç¤ºçˆ¬å–å®Œæ¯•
# ä¸‹ä¸€é¡µ <a class="page next"  href="https://travel.qunar.com/p-oi710603-gugong-1-2?rank=0#lydp">ä¸‹ä¸€é¡µ</a>
# ç”¨æˆ·ID <li class="e_comment_item clrfix" id="cmt_item_10158224437">
# ç”¨æˆ·è¯„è®º <p class="first">æˆ‘å’ŒåŒå­¦æ˜¯10æœˆ1å·å»çš„ï¼Œå› ä¸ºäººå¤ªå¤šï¼Œ</p>

import requests
import lxml
import xlrd
import re
import urllib
import time
import random
import urllib.request as request
from bs4 import BeautifulSoup
from lxml import etree
#é˜²æ­¢ä½ æ‰€åœ¨çš„IPè®¿é—®é¢‘ç‡è¿‡é«˜
user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36 Edg/81.0.416.72'
headers = {'User-Agent': user_agent}
proxies = {"http": "http://198.199.120.102:8080"}

def level2page(urls):#äºŒçº§é¡µé¢
    time.sleep(random.random() * 3)
    comment=[]#éœ€è¦çš„è¯„è®º
    comment_url=set()#æ²¡æœ‰æ˜¾ç¤ºå…¨æ–‡çš„è¯„è®ºé“¾æ¥id
    comment_id=set()#æ‰€æœ‰è¯„è®ºçš„id
    flag=0
  #  comment_url.append(urls)
    req = request.Request(url, headers=headers)
    try:#å› ä¸ºæœ‰å¯èƒ½é¡µé¢ä¸å­˜åœ¨, 404
        page = request.urlopen(req).read().decode('utf-8','ignore')
        print(page)
    except:
        flag=1
        pass
    if(flag):
        flag=0
        return
    bs_text = BeautifulSoup(page, 'lxml')
    all_comment = bs_text.find_all('p')
    print(all_comment)
    for i in all_comment:
        tmp = i.get_text()
        print(tmp)

'''
    nextpage = bs_text.select('js_replace_box > div.b_comment_box > div > a.page.next')
    nextpage = re.findall(r"[a-zA-z]+://[^\s]*", str(nextpage))
    nextpage = str(nextpage[1])
    nextpage = nextpage[:-10]
    # print(nextpage)
    level2page(nextpage)
'''

'''æš‚æ—¶é˜‰å‰²äº†
    #1.æœç´¢Seemore, åŒ¹é…çš„åœ°å€æœ€åçš„æ•°å­—å³ comment
    all_comment = bs_text.find_all('li',attrs={'class':'e_comment_item clrfix'})
    n=0
    print(urls,"\n")
    all_comment=etree.parse(urls)
    sb=etree.tostring(all_comment,pretty_print=True)
    sx=sb.decode('utf-8')
    sd=sx.xpath('/html/body/div[2]/div/div[1]/div[2]/div[5]/div[2]/div[8]/div[4]/div[2]/ul/li[5]')
    print("YES",sd)



    for i in all_li:
      #find comment_url
      tmp=re.search(r"href(.*)",str(all_li[n]))
      tmp=re.search(r"[a-zA-z]+://[^\s]*",str(tmp.group()))
      tmp = str(tmp.group())[:-1]
      if(36<len(tmp)):
           continue
      tmp=tmp[-7:]
      comment_url.add(tmp)
      n=n+1
    n=0
    for i in all_li:
      #find comment_id
      id = re.findall(r"id(.*)", str(all_li[n]))
      id=str(id)
      id=id[13:20]
      comment_id.add(id)
      n=n+1
  # print("è¯„è®ºé“¾æ¥: ",comment_url)
  # print("æ‰€æœ‰è¯„è®ºid",comment_id)

  #2.æå–è¯„è®º,æ£€æŸ¥æ˜¯å¦æœ‰"é˜…è¯»å…¨éƒ¨"=1 comment id ä¸Seemoreç½‘å€æœ«å°¾çš„idç›¸åŒ è¿›å…¥ä¸‰çº§é¡µé¢
    for full_text in comment_url:
      page3="https://travel.qunar.com/p-pl"+full_text
     # print(page3)
      level3page(page3)
  #"æ²¡æœ‰é˜…è¯»å…¨éƒ¨"=0,åœ¨äºŒçº§é¡µé¢ç›´æ¥æŠ“å–
    # ä¸‹ä¸€è¯„è®ºé¡µé¢
    nextpage = bs_text.select('js_replace_box > div.b_comment_box > div > a.page.next')
    nextpage = re.findall(r"[a-zA-z]+://[^\s]*", str(nextpage))
    nextpage = str(nextpage[1])
    nextpage = nextpage[:-10]
    # print(nextpage)
    level2page(nextpage)


def level3page(urls):
    req = request.Request(urls, headers=headers)
    global pa
    try:
     pa = request.urlopen(req).read().decode('utf-8')
    except :
        pass
    bs_text = BeautifulSoup(pa, 'lxml')
    all_div=bs_text.select('#js_mainleft > div.b_comment_detail_box > div.b_comment_detail > div.e_comment_content_box > div')
    #print("all_div",all_div)
    divs = bs_text.find_all('p', attrs={'class': 'first'})#ç›´æ¥è·å–å…¨æ–‡çš„è¯„è®º
    for i in divs:
        tmp=i.get_text()
       # print(tmp)
 '''


#è·å–htmlæ–‡æœ¬
for pages in range(1,2):
    time.sleep(random.random() * 3)
    print("æ­£åœ¨æŠ“å–ç¬¬",pages,"é¡µ")
    url = 'https://travel.qunar.com/p-cs299914-beijing-jingdian-1-1' #å¯¹äºåŒ—äº¬çš„æ™¯ç‚¹é¦–é¡µ
    #      https://travel.qunar.com/p-cs299914-beijing-jingdian-1-n å…¶ä¸­nä¸ºç¬¬né¡µ æ¯æ¬¡å¢åŠ 1
    url=url[:-1]
    url=url+str(pages)

    req = request.Request(url, headers=headers)
    pag= request.urlopen(req).read().decode('utf-8')
    bs_text=BeautifulSoup(pag,'lxml')

    #ä¸€çº§é¡µé¢
    all_li=bs_text.select('body > div.qn_main_box > div > div.qn_main_ct.clrfix > div.qn_main_ct_l > div > div.listbox > ul > li > div > div.titbox.clrfix > a')
    all_inform=bs_text.select('body > div.qn_main_box > div > div.qn_main_ct.clrfix > div.qn_main_ct_l > div > div.listbox > ul > li > div > div.txtbox.clrfix > div.desbox')
    # print(all_li)
    # print(all_inform)
    name=[]
    inform=[]
    url2=[]
    # print("è·å–æ™¯ç‚¹åç§°:")
    # for i in all_li:
    #     tmp=i.get_text()
    #     name.append(tmp)
    # print(name)
    # print("è·å–æ™¯ç‚¹è¯„è®ºé“¾æ¥")
    n=0
    for i in all_li:
        tmp=re.search(r"href(.*)",str(all_li[n]))#æ‰¾åˆ°ç½‘é¡µé“¾æ¥æ‰€åœ¨åŠä¹‹åçš„å­—ç¬¦ä¸²
        tmp=re.search(r"[a-zA-z]+://[^\s]*",str(tmp.group()))#åŒ¹é…ç½‘é¡µé“¾æ¥
        tmp=str(tmp.group())[:-1]#å»æ‰å¤šä½™çš„å•å¼•å·
     #   url2.append(tmp)#ğŸ‘†æ¸…æ´—æ™¯ç‚¹è¯¦æƒ…é¡µé¢çš„é“¾æ¥
        level2page(tmp)#è¿›å…¥æ™¯ç‚¹è¯„è®ºçš„çˆ¬å–
        n=n+1#re.searchæ‰¾çš„æ˜¯ç¬¬ä¸€ä¸ª, næ¯æ¬¡åŠ ä¸€, å³searchçš„ç»“æœåç§»
    # print(url2)
    # print("è·å–æ™¯ç‚¹ä»‹ç»:")
    # for i in all_inform:
    #     tmp=i.get_text()
    #     if(1 > len(tmp)):#æ²¡æœ‰æ™¯ç‚¹ä»‹ç»
    #         tmp='NA'
    #     inform.append(tmp)
    # print(inform)


'''
test=etree.HTML(url)
t=test.xpath('//li[@class="item"/div/div.titbox.clrfix/a/@href')
print(t)

for all_li in all_li:
    print('li:\n')
    print('li text:',all_li.text)
all_span=bs_text.find_all('span')

desbox=bs_text.find_all('div',attrs={'class':'desbox'})
print(desbox)

next_page=bs_text.find_all(href=requests.compile(r'https://travel.qunar.com/p-'))#è·å–äºŒçº§é¡µé¢é“¾æ¥ä¸›

for next_page in next_page:
    print('next_page:')
    print(next_page)

'''



